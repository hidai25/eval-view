# Reference Agent for EvalView Testing

## Overview

A simple FastAPI-based test agent with multiple tools to validate EvalView compatibility. This serves as:
1. **Validation** - Ensure EvalView core functionality works
2. **Template** - Reference implementation for testing other frameworks
3. **Debugging** - Isolated environment to test changes

## Features

### Available Tools

- **calculator** - Basic arithmetic (add, subtract, multiply, divide)
- **get_weather** - Weather lookup for major cities
- **search_web** - Web search simulation
- **convert_temperature** - Celsius/Fahrenheit conversion
- **get_stock_price** - Stock price lookup (AAPL, GOOGL, MSFT)

### Test Coverage

âœ… Simple single-tool calls
âœ… Multi-tool sequences
âœ… Error handling
âœ… Cost tracking
âœ… Latency measurement

## Quick Start

### 1. Install Dependencies

```bash
cd agent-testing/reference-agent
pip install -r requirements.txt
```

### 2. Start the Agent

```bash
python agent.py
```

Expected output:
```
ğŸš€ Starting Reference Test Agent on http://localhost:8000
ğŸ“š API docs available at http://localhost:8000/docs
ğŸ”§ Available tools: ['calculator', 'get_weather', 'search_web', 'convert_temperature', 'get_stock_price']
```

### 3. Test the Agent Manually (Optional)

```bash
# Health check
curl http://localhost:8000/health

# List tools
curl http://localhost:8000/tools

# Test execution
curl -X POST http://localhost:8000/execute \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "user", "content": "What is 5 plus 3?"}
    ]
  }'
```

### 4. Run EvalView Tests

In a new terminal:

```bash
# From project root
cd agent-testing/reference-agent
evalview run
```

## Test Cases

| Test Case | Description | Tools | Expected Score |
|-----------|-------------|-------|----------------|
| 01-simple-calculator | Basic addition | calculator | 80+ |
| 02-weather-query | Weather lookup | get_weather | 75+ |
| 03-multi-tool-sequence | Weather + temp conversion | get_weather, convert_temperature | 70+ |
| 04-error-handling | Invalid city handling | get_weather | 70+ |
| 05-stock-query | Stock price lookup | get_stock_price | 75+ |
| 06-multiplication | Multiplication | calculator | 80+ |

## Expected Results

All tests should **PASS** with:
- âœ… Correct tool calls detected
- âœ… Tool sequences validated
- âœ… Output contains expected strings
- âœ… Cost and latency within thresholds

### Sample Output

```
ğŸ§ª EvalView Test Results

âœ… Simple Calculator - Addition (score: 95.0)
   Tools: calculator âœ“
   Cost: $0.0005 | Latency: 45ms

âœ… Weather Query - Single Tool (score: 92.5)
   Tools: get_weather âœ“
   Cost: $0.001 | Latency: 52ms

âœ… Multi-Tool Sequence - Weather & Conversion (score: 88.0)
   Tools: get_weather âœ“, convert_temperature âœ“
   Sequence: correct âœ“
   Cost: $0.002 | Latency: 78ms

...

Overall: 6/6 tests passed
```

## API Reference

### POST /execute

Execute agent with user query.

**Request:**
```json
{
  "messages": [
    {"role": "user", "content": "Your query here"}
  ]
}
```

**Response:**
```json
{
  "output": "Agent response text",
  "tool_calls": [
    {
      "name": "calculator",
      "arguments": {"operation": "add", "a": 5, "b": 3},
      "result": 8
    }
  ],
  "cost": 0.0005,
  "latency": 45.2
}
```

### GET /health

Health check endpoint.

### GET /tools

List available tools and their descriptions.

## Agent Logic

The reference agent uses **simple rule-based logic** to determine tool calls:

- Weather keywords â†’ `get_weather`
- Math keywords (add, plus, multiply, times) â†’ `calculator`
- Stock keywords or symbols â†’ `get_stock_price`
- Temperature conversion â†’ `convert_temperature`
- Fallback â†’ `search_web`

In production agents, this logic would be replaced by an LLM deciding which tools to use.

## Customization

### Adding New Tools

```python
def my_new_tool(param: str) -> Any:
    """Tool description."""
    return "result"

# Register tool
TOOLS["my_new_tool"] = my_new_tool

# Add to agent logic in simple_agent_logic()
```

### Modifying Agent Behavior

Edit `simple_agent_logic()` to change:
- Tool selection criteria
- Cost calculations
- Response formatting
- Error handling

## Troubleshooting

### Port Already in Use

```bash
# Kill process on port 8000
lsof -ti:8000 | xargs kill -9

# Or use a different port
python agent.py --port 8001
```

### EvalView Connection Error

- Ensure agent is running on http://localhost:8000
- Check `.evalview/config.yaml` has correct endpoint
- Verify firewall isn't blocking localhost

### Tests Failing

- Check agent logs for errors
- Run tests with `evalview run --verbose`
- Verify test case YAML syntax
- Check tool call names match exactly

## Next Steps

Once reference agent tests pass:

1. âœ… **Validate** - EvalView core functionality works
2. ğŸ“ **Document** - Known issues or limitations
3. ğŸ”„ **Template** - Use this structure for other frameworks
4. ğŸš€ **Test More** - Move on to LangChain, LangGraph, etc.

## Framework Testing Template

Use this structure for each framework:

```
agent-testing/{framework}/
â”œâ”€â”€ README.md              # Setup instructions
â”œâ”€â”€ agent.py              # Agent implementation
â”œâ”€â”€ requirements.txt      # Framework dependencies
â”œâ”€â”€ .evalview/
â”‚   â””â”€â”€ config.yaml      # Adapter configuration
â””â”€â”€ test-cases/
    â”œâ”€â”€ 01-simple.yaml   # Basic test
    â”œâ”€â”€ 02-multi.yaml    # Complex test
    â””â”€â”€ 03-error.yaml    # Error handling
```

## Contributing

Found issues testing with this reference agent?
1. Document the issue in AGENT_TESTING.md
2. Create a minimal reproduction
3. Open an issue with "[Testing]" prefix
