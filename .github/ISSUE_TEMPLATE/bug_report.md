---
name: Bug Report
about: Create a report to help us improve EvalView
title: '[BUG] '
labels: bug
assignees: ''
---

## Bug Description

A clear and concise description of what the bug is.

## To Reproduce

Steps to reproduce the behavior:

1. Set up test case with '...'
2. Run command '....'
3. See error

## Expected Behavior

A clear and concise description of what you expected to happen.

## Actual Behavior

What actually happened.

## Test Case (if applicable)

Please provide the YAML test case that reproduces the issue:

```yaml
# Your test case here
```

## Error Output

```
Paste any error messages or stack traces here
```

## Environment

- **EvalView Version**: [e.g., 0.1.0]
- **Python Version**: [e.g., 3.11.0]
- **Operating System**: [e.g., macOS 14.0, Ubuntu 22.04, Windows 11]
- **Agent Framework**: [e.g., LangGraph, CrewAI, OpenAI Assistants]

## Configuration

- **Adapter Used**: [e.g., HTTPAdapter, LangGraphAdapter]
- **OpenAI Model** (if using LLM-as-judge): [e.g., gpt-4, gpt-3.5-turbo]

## Additional Context

Add any other context about the problem here, such as:
- Does this happen consistently or intermittently?
- Did this work in a previous version?
- Any relevant configuration files?

## Possible Solution

If you have ideas on how to fix this, please share them here.
