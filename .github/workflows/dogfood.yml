name: Daily Dogfood

on:
  schedule:
    # Run daily at 9am UTC (adjusts for timezones)
    - cron: '0 9 * * *'
  workflow_dispatch:  # Allow manual trigger

permissions:
  contents: read
  issues: write

jobs:
  dogfood:
    name: Dogfood Tests
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: "3.11"

    - name: Install uv
      run: pip install uv

    - name: Install dependencies
      run: uv sync --all-extras

    - name: Run pytest (unit tests)
      id: pytest
      continue-on-error: true
      run: |
        uv run pytest tests/ -v --tb=short -k "not LLM and not Hallucination" 2>&1 | tee pytest-output.txt
        echo "exit_code=${PIPESTATUS[0]}" >> $GITHUB_OUTPUT

    - name: Run pytest (LLM evaluator tests)
      id: pytest_llm
      continue-on-error: true
      env:
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
      run: |
        if [ -z "$OPENAI_API_KEY" ]; then
          echo "Skipping LLM evaluator tests - no OPENAI_API_KEY"
          echo "exit_code=0" >> $GITHUB_OUTPUT
          echo "skipped=true" >> $GITHUB_OUTPUT
          exit 0
        fi
        uv run pytest tests/ -v --tb=short -k "LLM or Hallucination" 2>&1 | tee pytest-llm-output.txt
        echo "exit_code=${PIPESTATUS[0]}" >> $GITHUB_OUTPUT

    - name: Run mypy
      id: mypy
      continue-on-error: true
      run: |
        uv run mypy evalview/ 2>&1 | tee mypy-output.txt
        echo "exit_code=${PIPESTATUS[0]}" >> $GITHUB_OUTPUT

    - name: Run evalview demo
      id: demo
      continue-on-error: true
      run: |
        uv run evalview demo 2>&1 | tee demo-output.txt
        echo "exit_code=${PIPESTATUS[0]}" >> $GITHUB_OUTPUT

    - name: Run evalview --help
      id: help
      continue-on-error: true
      run: |
        uv run evalview --help 2>&1 | tee help-output.txt
        echo "exit_code=${PIPESTATUS[0]}" >> $GITHUB_OUTPUT

    - name: Run e2e dogfood tests (mock agent)
      id: dogfood_e2e
      continue-on-error: true
      run: |
        # Install mock agent dependencies
        uv pip install fastapi uvicorn httpx

        # Run e2e dogfood tests (starts mock agent automatically via fixture)
        uv run pytest tests/test_dogfood_e2e.py -v --tb=short 2>&1 | tee dogfood-e2e-output.txt
        echo "exit_code=${PIPESTATUS[0]}" >> $GITHUB_OUTPUT

    - name: Run dogfood self-tests (chat mode)
      id: dogfood
      continue-on-error: true
      env:
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
      run: |
        # Skip if no API key
        if [ -z "$OPENAI_API_KEY" ]; then
          echo "Skipping chat mode dogfood tests - no OPENAI_API_KEY secret configured"
          echo "exit_code=0" >> $GITHUB_OUTPUT
          echo "skipped=true" >> $GITHUB_OUTPUT
          exit 0
        fi

        # Install dogfood agent dependencies
        uv pip install fastapi uvicorn python-dotenv

        # Create config directory and file
        mkdir -p .evalview
        cp dogfood/config.yaml .evalview/config.yaml

        # Start dogfood agent in background
        uv run python dogfood/agent.py &
        AGENT_PID=$!

        # Wait for agent to be ready
        for i in {1..30}; do
          if curl -s http://localhost:8001/health > /dev/null 2>&1; then
            echo "Dogfood agent ready"
            break
          fi
          sleep 1
        done

        # Run dogfood tests
        uv run evalview run dogfood/test-cases/ --sequential 2>&1 | tee dogfood-output.txt
        EXIT_CODE=${PIPESTATUS[0]}

        # Cleanup
        kill $AGENT_PID 2>/dev/null || true

        echo "exit_code=$EXIT_CODE" >> $GITHUB_OUTPUT

    - name: Check for failures
      id: check
      run: |
        FAILED=""

        if [ "${{ steps.pytest.outputs.exit_code }}" != "0" ]; then
          FAILED="${FAILED}pytest,"
        fi

        if [ "${{ steps.pytest_llm.outputs.exit_code }}" != "0" ] && [ "${{ steps.pytest_llm.outputs.skipped }}" != "true" ]; then
          FAILED="${FAILED}pytest-llm,"
        fi

        if [ "${{ steps.mypy.outputs.exit_code }}" != "0" ]; then
          FAILED="${FAILED}mypy,"
        fi

        if [ "${{ steps.demo.outputs.exit_code }}" != "0" ]; then
          FAILED="${FAILED}demo,"
        fi

        if [ "${{ steps.help.outputs.exit_code }}" != "0" ]; then
          FAILED="${FAILED}help,"
        fi

        if [ "${{ steps.dogfood_e2e.outputs.exit_code }}" != "0" ]; then
          FAILED="${FAILED}dogfood-e2e,"
        fi

        if [ "${{ steps.dogfood.outputs.exit_code }}" != "0" ] && [ "${{ steps.dogfood.outputs.skipped }}" != "true" ]; then
          FAILED="${FAILED}dogfood,"
        fi

        if [ -n "$FAILED" ]; then
          echo "failed=${FAILED%,}" >> $GITHUB_OUTPUT
          echo "has_failures=true" >> $GITHUB_OUTPUT
        else
          echo "has_failures=false" >> $GITHUB_OUTPUT
        fi

    - name: Create issue on failure
      if: steps.check.outputs.has_failures == 'true'
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');

          const failed = '${{ steps.check.outputs.failed }}'.split(',');
          const date = new Date().toISOString().split('T')[0];

          let body = `## Daily Dogfood Failed - ${date}\n\n`;
          body += `The following checks failed:\n\n`;

          for (const check of failed) {
            body += `### âŒ ${check}\n\n`;
            try {
              const output = fs.readFileSync(`${check}-output.txt`, 'utf8');
              // Truncate if too long
              const truncated = output.length > 3000
                ? output.slice(-3000) + '\n...(truncated)'
                : output;
              body += '```\n' + truncated + '\n```\n\n';
            } catch (e) {
              body += `Could not read output file.\n\n`;
            }
          }

          body += `---\n`;
          body += `ðŸ”— [View workflow run](${process.env.GITHUB_SERVER_URL}/${process.env.GITHUB_REPOSITORY}/actions/runs/${process.env.GITHUB_RUN_ID})`;

          // Try to create dogfood label if it doesn't exist
          try {
            await github.rest.issues.createLabel({
              owner: context.repo.owner,
              repo: context.repo.repo,
              name: 'dogfood',
              color: 'FFA500',
              description: 'Daily self-testing failures'
            });
          } catch (e) {
            // Label likely already exists
          }

          await github.rest.issues.create({
            owner: context.repo.owner,
            repo: context.repo.repo,
            title: `ðŸ• Dogfood failed: ${failed.join(', ')} (${date})`,
            body: body,
            labels: ['dogfood']
          });

    - name: Summary
      run: |
        echo "## Dogfood Results" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY

        if [ "${{ steps.pytest.outputs.exit_code }}" == "0" ]; then
          echo "âœ… pytest (unit tests) passed" >> $GITHUB_STEP_SUMMARY
        else
          echo "âŒ pytest (unit tests) failed" >> $GITHUB_STEP_SUMMARY
        fi

        if [ "${{ steps.pytest_llm.outputs.skipped }}" == "true" ]; then
          echo "â­ï¸ pytest (LLM evaluator tests) skipped (no API key)" >> $GITHUB_STEP_SUMMARY
        elif [ "${{ steps.pytest_llm.outputs.exit_code }}" == "0" ]; then
          echo "âœ… pytest (LLM evaluator tests) passed" >> $GITHUB_STEP_SUMMARY
        else
          echo "âŒ pytest (LLM evaluator tests) failed" >> $GITHUB_STEP_SUMMARY
        fi

        if [ "${{ steps.mypy.outputs.exit_code }}" == "0" ]; then
          echo "âœ… mypy passed" >> $GITHUB_STEP_SUMMARY
        else
          echo "âŒ mypy failed" >> $GITHUB_STEP_SUMMARY
        fi

        if [ "${{ steps.demo.outputs.exit_code }}" == "0" ]; then
          echo "âœ… evalview demo passed" >> $GITHUB_STEP_SUMMARY
        else
          echo "âŒ evalview demo failed" >> $GITHUB_STEP_SUMMARY
        fi

        if [ "${{ steps.help.outputs.exit_code }}" == "0" ]; then
          echo "âœ… evalview --help passed" >> $GITHUB_STEP_SUMMARY
        else
          echo "âŒ evalview --help failed" >> $GITHUB_STEP_SUMMARY
        fi

        if [ "${{ steps.dogfood_e2e.outputs.exit_code }}" == "0" ]; then
          echo "âœ… e2e dogfood tests passed (12 tests)" >> $GITHUB_STEP_SUMMARY
        else
          echo "âŒ e2e dogfood tests failed" >> $GITHUB_STEP_SUMMARY
        fi

        if [ "${{ steps.dogfood.outputs.skipped }}" == "true" ]; then
          echo "â­ï¸ dogfood self-tests skipped (no API key)" >> $GITHUB_STEP_SUMMARY
        elif [ "${{ steps.dogfood.outputs.exit_code }}" == "0" ]; then
          echo "âœ… dogfood self-tests passed" >> $GITHUB_STEP_SUMMARY
        else
          echo "âŒ dogfood self-tests failed" >> $GITHUB_STEP_SUMMARY
        fi
