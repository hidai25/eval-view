# EvalView Examples

Real-world examples of testing the **5 most popular AI agent frameworks** with EvalView.

## Quick Start

1. Choose an example below
2. Follow setup instructions in each folder
3. Run: `evalview run --pattern examples/<folder>/test-case.yaml`

## Examples

| Framework | Stars | Use Case | Folder |
|-----------|-------|----------|--------|
| ðŸ¦œ LangGraph | 8k+ | Multi-step research agent | [langgraph/](langgraph/) |
| ðŸš¢ CrewAI | 25k+ | Multi-agent team collaboration | [crewai/](crewai/) |
| ðŸ¤– AutoGen | 35k+ | Multi-agent conversations | [autogen/](autogen/) |
| ðŸŽ¨ Dify | 55k+ | Visual workflow builder | [dify/](dify/) |
| ðŸ’¬ OpenAI Assistants | - | Native OpenAI agents | [openai-assistants/](openai-assistants/) |

## Screenshots

See actual EvalView output for each framework:

![LangGraph](../assets/langgraph-output.png)
![CrewAI](../assets/crewai-output.png)
![AutoGen](../assets/autogen-output.png)
![Dify](../assets/dify-output.png)
![OpenAI Assistants](../assets/openai-assistants-output.png)

## Quick Clone Commands

```bash
# LangGraph
git clone https://github.com/langchain-ai/langgraph.git

# CrewAI
git clone https://github.com/crewAIInc/crewAI-examples.git

# AutoGen
git clone https://github.com/microsoft/autogen.git

# Dify
git clone https://github.com/langgenius/dify.git

# OpenAI Assistants - No clone needed, use API directly
```
