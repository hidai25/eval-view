#!/usr/bin/env python3
"""Generate a viral scorecard from GooseBench results.

Usage:
    python scorecard.py .evalview/results/latest.json
    python scorecard.py .evalview/results/latest.json --output scorecard.md
"""

import argparse
import json
import subprocess
import sys
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List


def get_goose_version() -> str:
    """Get installed Goose version."""
    try:
        result = subprocess.run(
            ["goose", "--version"],
            capture_output=True,
            text=True,
            timeout=5,
        )
        if result.returncode == 0:
            return result.stdout.strip()
        return "unknown"
    except Exception:
        return "unknown"


def load_results(path: str) -> List[Dict[str, Any]]:
    """Load results from JSON file."""
    with open(path) as f:
        data = json.load(f)

    # Handle both single result and array of results
    if isinstance(data, list):
        return data
    elif isinstance(data, dict):
        if "results" in data:
            return data["results"]
        return [data]
    return []


def generate_scorecard(results: List[Dict[str, Any]]) -> str:
    """Generate markdown scorecard from results."""
    total = len(results)
    passed = sum(1 for r in results if r.get("passed", False))
    failed = total - passed

    goose_version = get_goose_version()
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M")

    # Header
    lines = [
        "# GooseBench Results",
        "",
        f"**Goose Version:** {goose_version}",
        f"**Run Date:** {timestamp}",
        "",
        "---",
        "",
    ]

    # Summary with emoji
    if passed == total:
        emoji = "✅"
        status = "All tasks passed!"
    elif passed >= total * 0.7:
        emoji = "⚠️"
        status = "Some regressions detected"
    else:
        emoji = "❌"
        status = "Significant issues found"

    lines.extend([
        f"## {emoji} {passed}/{total} tasks passed",
        "",
        f"*{status}*",
        "",
    ])

    # Failures detail
    failures = [r for r in results if not r.get("passed", False)]
    if failures:
        lines.extend([
            "### Regressions Detected",
            "",
        ])

        for result in failures:
            test_name = result.get("test_case", "Unknown")
            evaluations = result.get("evaluations", {})

            # Get tool accuracy info
            tool_eval = evaluations.get("tool_accuracy", {})
            missing_tools = tool_eval.get("missing", [])
            actual_tools = tool_eval.get("correct", []) + tool_eval.get("unexpected", [])

            # Get sequence info
            seq_eval = evaluations.get("sequence_correctness", {})
            expected_seq = seq_eval.get("expected_sequence", [])
            actual_seq = seq_eval.get("actual_sequence", [])

            lines.append(f"**❌ {test_name}**")

            if missing_tools:
                lines.append(f"- Expected tools: `{missing_tools}`")
                lines.append(f"- Actual tools: `{actual_tools if actual_tools else '[]'}`")
                if not actual_tools:
                    lines.append("- → **Answered without using tools (hallucination)**")

            lines.append("")

    # Passes summary
    passes = [r for r in results if r.get("passed", False)]
    if passes:
        lines.extend([
            "### Passed",
            "",
        ])
        for result in passes:
            test_name = result.get("test_case", "Unknown")
            score = result.get("score", 0)
            lines.append(f"✅ {test_name} (score: {score:.0f})")
        lines.append("")

    # Cost summary
    total_cost = sum(
        r.get("trace", {}).get("metrics", {}).get("total_cost", 0)
        for r in results
    )
    total_latency = sum(
        r.get("trace", {}).get("metrics", {}).get("total_latency", 0)
        for r in results
    )

    lines.extend([
        "---",
        "",
        "### Metrics",
        "",
        f"- **Total Cost:** ${total_cost:.4f}",
        f"- **Total Latency:** {total_latency/1000:.1f}s",
        "",
        "---",
        "",
        "*Generated by [GooseBench](https://github.com/hidai25/EvalView) - Regression tests for AI agents*",
    ])

    return "\n".join(lines)


def generate_compact_scorecard(results: List[Dict[str, Any]]) -> str:
    """Generate a compact scorecard for social media."""
    total = len(results)
    passed = sum(1 for r in results if r.get("passed", False))

    goose_version = get_goose_version()

    lines = [
        "```",
        f"GooseBench v0.1 | {goose_version}",
        "",
        f"{'✅' if passed == total else '⚠️'} {passed}/{total} tasks passed",
    ]

    # Show failures
    failures = [r for r in results if not r.get("passed", False)]
    for result in failures[:3]:  # Show max 3 failures
        test_name = result.get("test_case", "Unknown")
        tool_eval = result.get("evaluations", {}).get("tool_accuracy", {})
        missing = tool_eval.get("missing", [])

        if missing and not tool_eval.get("correct", []):
            lines.append(f"❌ {test_name}: answered without checking")
        else:
            lines.append(f"❌ {test_name}")

    if len(failures) > 3:
        lines.append(f"   ...and {len(failures) - 3} more")

    lines.extend([
        "```",
    ])

    return "\n".join(lines)


def main():
    parser = argparse.ArgumentParser(description="Generate GooseBench scorecard")
    parser.add_argument("results_file", help="Path to results JSON file")
    parser.add_argument("--output", "-o", help="Output file path")
    parser.add_argument("--compact", action="store_true", help="Generate compact version for social media")

    args = parser.parse_args()

    if not Path(args.results_file).exists():
        print(f"Error: Results file not found: {args.results_file}")
        sys.exit(1)

    results = load_results(args.results_file)

    if not results:
        print("Error: No results found in file")
        sys.exit(1)

    if args.compact:
        scorecard = generate_compact_scorecard(results)
    else:
        scorecard = generate_scorecard(results)

    if args.output:
        with open(args.output, "w") as f:
            f.write(scorecard)
        print(f"Scorecard written to: {args.output}")
    else:
        print(scorecard)


if __name__ == "__main__":
    main()
