name: 'EvalView - AI Agent Testing'
description: 'Catch agent regressions in CI. Detects tool changes, output drift, cost spikes. Works with LangGraph, CrewAI, Claude, GPT.'
author: 'EvalView Team'

branding:
  icon: 'check-circle'
  color: 'purple'

inputs:
  openai-api-key:
    description: 'OpenAI API key for LLM-as-judge evaluation'
    required: false
  anthropic-api-key:
    description: 'Anthropic API key (optional alternative to OpenAI)'
    required: false
  config-path:
    description: 'Path to EvalView config file'
    required: false
    default: '.evalview/config.yaml'
  filter:
    description: 'Filter tests by name pattern'
    required: false
  max-workers:
    description: 'Maximum parallel workers'
    required: false
    default: '4'
  max-retries:
    description: 'Maximum retries for failed tests'
    required: false
    default: '2'
  fail-on-error:
    description: 'Fail the workflow if any test fails'
    required: false
    default: 'true'
  diff:
    description: 'Compare against golden baselines for regression detection'
    required: false
    default: 'false'
  fail-on:
    description: 'Comma-separated statuses that fail CI (REGRESSION, TOOLS_CHANGED, OUTPUT_CHANGED)'
    required: false
    default: 'REGRESSION'
  generate-report:
    description: 'Generate HTML report'
    required: false
    default: 'true'
  python-version:
    description: 'Python version to use'
    required: false
    default: '3.11'

outputs:
  results-file:
    description: 'Path to JSON results file'
    value: ${{ steps.run-tests.outputs.results-file }}
  report-file:
    description: 'Path to HTML report file'
    value: ${{ steps.run-tests.outputs.report-file }}
  total-tests:
    description: 'Total number of tests run'
    value: ${{ steps.run-tests.outputs.total-tests }}
  passed-tests:
    description: 'Number of passed tests'
    value: ${{ steps.run-tests.outputs.passed-tests }}
  failed-tests:
    description: 'Number of failed tests'
    value: ${{ steps.run-tests.outputs.failed-tests }}
  pass-rate:
    description: 'Pass rate percentage'
    value: ${{ steps.run-tests.outputs.pass-rate }}

runs:
  using: 'composite'
  steps:
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ inputs.python-version }}

    - name: Install EvalView
      shell: bash
      run: |
        python -m pip install --upgrade pip
        pip install evalview

    - name: Run EvalView Tests
      id: run-tests
      shell: bash
      env:
        OPENAI_API_KEY: ${{ inputs.openai-api-key }}
        ANTHROPIC_API_KEY: ${{ inputs.anthropic-api-key }}
      run: |
        # Build command
        CMD="evalview run"

        if [ -n "${{ inputs.filter }}" ]; then
          CMD="$CMD --filter '${{ inputs.filter }}'"
        fi

        CMD="$CMD --max-workers ${{ inputs.max-workers }}"
        CMD="$CMD --max-retries ${{ inputs.max-retries }}"

        # Regression detection mode
        if [ "${{ inputs.diff }}" = "true" ]; then
          CMD="$CMD --diff --fail-on ${{ inputs.fail-on }}"
        fi

        if [ "${{ inputs.generate-report }}" = "true" ]; then
          CMD="$CMD --html-report evalview-report.html"
        fi

        # Run tests and capture output
        set +e
        eval $CMD
        EXIT_CODE=$?
        set -e

        # Find results file
        RESULTS_FILE=$(ls -t .evalview/results/*.json 2>/dev/null | head -1)

        if [ -n "$RESULTS_FILE" ]; then
          echo "results-file=$RESULTS_FILE" >> $GITHUB_OUTPUT

          # Parse results
          TOTAL=$(jq '.summary.total // 0' "$RESULTS_FILE")
          PASSED=$(jq '.summary.passed // 0' "$RESULTS_FILE")
          FAILED=$(jq '.summary.failed // 0' "$RESULTS_FILE")

          if [ "$TOTAL" -gt 0 ]; then
            PASS_RATE=$(echo "scale=1; $PASSED * 100 / $TOTAL" | bc)
          else
            PASS_RATE="0"
          fi

          echo "total-tests=$TOTAL" >> $GITHUB_OUTPUT
          echo "passed-tests=$PASSED" >> $GITHUB_OUTPUT
          echo "failed-tests=$FAILED" >> $GITHUB_OUTPUT
          echo "pass-rate=$PASS_RATE" >> $GITHUB_OUTPUT
        fi

        if [ -f "evalview-report.html" ]; then
          echo "report-file=evalview-report.html" >> $GITHUB_OUTPUT
        fi

        # Handle exit code based on fail-on-error setting
        if [ "${{ inputs.fail-on-error }}" = "true" ] && [ $EXIT_CODE -ne 0 ]; then
          exit $EXIT_CODE
        fi
